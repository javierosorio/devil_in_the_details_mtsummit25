{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Ykfhp29jc9QR",
      "metadata": {
        "id": "Ykfhp29jc9QR"
      },
      "source": [
        "# coling2025_ling_analysis.ipynb\n",
        "### Author: Amber Charlotte Converse\n",
        "### Purpose: This file contains cells for processing text from data_master_text.csv to data files containing statistics on linguistic features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I63CWDhL_bGH",
      "metadata": {
        "id": "I63CWDhL_bGH"
      },
      "outputs": [],
      "source": [
        "# Run if running in Google Colab to link Google Drive file system\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "565c0bdf",
      "metadata": {
        "id": "565c0bdf"
      },
      "outputs": [],
      "source": [
        "# Library installs (if not already installed)\n",
        "#!pip install numpy==1.24\n",
        "!pip install pandas\n",
        "#!pip install -U pip setuptools wheel\n",
        "!pip install spacy==3.7.6\n",
        "!python3 -m spacy download en_core_web_trf\n",
        "!python3 -m spacy download es_dep_news_trf\n",
        "!pip install farasapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c31ad8",
      "metadata": {
        "id": "c7c31ad8"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import time\n",
        "import pickle\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "from multiprocessing import Pool\n",
        "from farasa.pos import FarasaPOSTagger\n",
        "from farasa.stemmer import FarasaStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y1mvQ4kU6hGg",
      "metadata": {
        "id": "Y1mvQ4kU6hGg"
      },
      "outputs": [],
      "source": [
        "def generate_spacy(text):\n",
        "  '''\n",
        "  Generates a SpaCy document from text in json format for pickling.\n",
        "  Requires three global variables: i (0 at calling apply), num_rows (length of column), and start_time (time.time() at calling apply)\n",
        "  These global variables are required for progress reporting.\n",
        "\n",
        "  :param: text (string): the sentence/paragraph to be processed using SpaCy.\n",
        "  :return: SpaCy document in json format\n",
        "  '''\n",
        "  global i\n",
        "  global num_rows\n",
        "  i += 1\n",
        "\n",
        "  if i % 500 == 0:\n",
        "      cur_time = time.time()\n",
        "      print(f\"{i / num_rows * 100}% done.\")\n",
        "      print(f\"Estimated time remaining: {((cur_time - start_time) / i) * (num_rows - i) / 60} minutes\")\n",
        "\n",
        "  try:\n",
        "      return nlp(text).to_json()\n",
        "  except Exception as e:\n",
        "      print(f\"Error on line {i}: {e}\")\n",
        "      return nlp(\"\").to_json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "926a8b92",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define file system:\n",
        "\n",
        "path_to_data_master = \"\"\n",
        "path_to_pickles = \"\"\n",
        "path_to_lang_vocab_lists = \"\"\n",
        "path_to_analysis_data = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wl6lbVIH-EyQ",
      "metadata": {
        "id": "wl6lbVIH-EyQ"
      },
      "outputs": [],
      "source": [
        "# Run this cell to generate SpaCy documents for all columns specified in the array langs in data_master.csv\n",
        "df = pd.read_csv(path_to_data_master)\n",
        "\n",
        "nlp = spacy.load(\"Language model to use here\") # Define SpaCy model to use, should be en_core_new_trf for English and es_dep_news_trf for Spanish\n",
        "\n",
        "langs = [\"columns\", \"to\", \"process\"] # All columns to be processed (should be all one language, otherwise nlp model will fail or process incorrectly\n",
        "\n",
        "i = 0\n",
        "num_rows = len(df)\n",
        "\n",
        "global start_time\n",
        "start_time = time.time()\n",
        "\n",
        "for lang in langs:\n",
        "  i = 0\n",
        "  res = list(df[lang].apply(generate_spacy))\n",
        "  with open(f\"{path_to_pickles}/{lang}_spacy.pickle\", 'wb') as f:\n",
        "    pickle.dump(res, f)\n",
        "    print(f\"Saved {lang}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jRMHgS_XbRjW",
      "metadata": {
        "id": "jRMHgS_XbRjW"
      },
      "outputs": [],
      "source": [
        "def count_pos(doc, parts_of_speech=[r\"NOUN.*\", r\"VERB.*\"]):\n",
        "  '''\n",
        "  Count parts of speech in the sentence represented by doc.\n",
        "\n",
        "  :param: doc (SpaCy Doc): the document to be analyzed\n",
        "  :param: parts_of_speech ([Str]): an array of regex strings to define parts of speech to be counted, by default counts nouns and verbs\n",
        "\n",
        "  :return: an array of integers representing the counts of each part of speech in the sentence, respective to the order of the regex array\n",
        "  '''\n",
        "  if doc.text == \"\":\n",
        "    return [None] * len(parts_of_speech)\n",
        "  counts = [0] * len(parts_of_speech)\n",
        "  for token in doc:\n",
        "    for i, part_of_speech in enumerate(parts_of_speech):\n",
        "      if re.match(part_of_speech, token.pos_):\n",
        "        counts[i] += 1\n",
        "  return counts\n",
        "\n",
        "def count_lemma(doc):\n",
        "  '''\n",
        "  Counts the number of unique lemmas in the sentence represented by doc.\n",
        "\n",
        "  :param: doc (SpaCy Doc): the document to be analyzed\n",
        "\n",
        "  :return: an integer representing the number of unique lemmas in the document\n",
        "  '''\n",
        "  return len(set([token.lemma_ for token in doc])) if doc.text != \"\" else None\n",
        "\n",
        "def count_all_lemma(doc):\n",
        "  '''\n",
        "  Adds all lemmas and words to sets for each language in the dictionaries lemmas and words. Used to define total number of unique lemmas and words\n",
        "\n",
        "  :param: doc (SpaCy Doc): the document to be analyzed\n",
        "\n",
        "  :return: None (effect is addition to the sets in lemmas)\n",
        "  '''\n",
        "  if doc.text == \"\":\n",
        "    return\n",
        "  for token in doc:\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", token.text)\n",
        "    if text != \"\":\n",
        "      lemmas[lang].add(token.lemma_)\n",
        "      words[lang].add(token.text)\n",
        "\n",
        "def measure_rarity(doc):\n",
        "  '''\n",
        "  Measure the number of tokens in the text which are rare in both a general corpus and political (genre) corpus.\n",
        "\n",
        "  :param: doc (SpaCy Doc): the document to be analyzed\n",
        "\n",
        "  :return: int, int: the number of rare tokens in the sentence as compared to a general corpus and the number of rare tokens in the sentence as compared to a political corpus\n",
        "  '''\n",
        "  general_rare_token_count = 0\n",
        "  genre_rare_token_count = 0\n",
        "  punct_count = 0\n",
        "  for token in doc:\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", token.text)\n",
        "    if text != \"\":\n",
        "      if not token.text in general_common_tokens:\n",
        "        general_rare_token_count += 1\n",
        "      if not token.text in genre_common_tokens:\n",
        "        genre_rare_token_count += 1\n",
        "    else:\n",
        "      punct_count += 1\n",
        "  return general_rare_token_count / (len(doc) - punct_count), genre_rare_token_count / (len(doc) - punct_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rllsTIJzckSN",
      "metadata": {
        "id": "rllsTIJzckSN"
      },
      "outputs": [],
      "source": [
        "# Analyze noun, verb, and lemma counts\n",
        "main_lang = \"es\" # Define language to analyze\n",
        "\n",
        "if main_lang == \"en\":\n",
        "  nlp = spacy.load(\"en_core_web_trf\")\n",
        "  langs = [\"en\", \"es_en_DEEP\", \"es_en_DEEPL\", \"es_en_GOOGLE\", \"es_en_TRANSFORMERS\", \\\n",
        "               \"ar_en_DEEP\", \"ar_en_DEEPL\", \"ar_en_GOOGLE\", \"ar_en_TRANSFORMERS\"]\n",
        "elif main_lang == \"es\":\n",
        "  nlp = spacy.load(\"es_dep_news_trf\")\n",
        "  langs = [\"es\", \"en_es_DEEP\", \"en_es_DEEPL\", \"en_es_GOOGLE\", \"en_es_TRANSFORMERS\"]\n",
        "# elif main_lang == \"ar\":\n",
        "#   # nlp = spacy.load(\"ar_dep_news_trf\")\n",
        "#   sub_langs = [\"en_ar_DEEP\", \"en_ar_DEEPL\", \"en_ar_GOOGLE\", \"en_ar_TRANSFORMERS\"]\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for lang in langs:\n",
        "  with open(f\"{path_to_pickles}/{lang}_spacy.pickle\", 'rb') as f:\n",
        "    df[lang] = pickle.load(f)\n",
        "  df[lang] = df[lang].apply(lambda x: spacy.tokens.Doc(nlp.vocab).from_json(x))\n",
        "\n",
        "master_df = pd.read_csv(path_to_data_master)\n",
        "out_df = pd.DataFrame(data={\"id\":master_df[\"id\"]})\n",
        "\n",
        "for lang in langs:\n",
        "  noun_counts, verb_counts = zip(*list(df[lang].apply(count_pos)))\n",
        "  out_df[f\"{lang}_noun_counts\"] = noun_counts\n",
        "  out_df[f\"{lang}_verb_counts\"] = verb_counts\n",
        "  out_df[f\"{lang}_lemma_counts\"] = df[lang].apply(count_lemma)\n",
        "\n",
        "out_df.to_csv(f\"{path_to_analysis_data}/{main_lang}_counts.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cH41RFzsnfIR",
      "metadata": {
        "id": "cH41RFzsnfIR"
      },
      "outputs": [],
      "source": [
        "# Count all unique lemmas and words in the corpus\n",
        "lemmas = {lang: set() for lang in langs}\n",
        "words = {lang: set() for lang in langs}\n",
        "\n",
        "corpus_counts_df = pd.DataFrame(data={\"label\":[\"lemma\",\"word\"]})\n",
        "\n",
        "for lang in langs:\n",
        "  df[lang].apply(count_all_lemma)\n",
        "\n",
        "for lang in lemmas.keys():\n",
        "  corpus_counts_df[lang] = [len(lemmas[lang]), len(words[lang])]\n",
        "\n",
        "corpus_counts_df.to_csv(f\"{path_to_analysis_data}/{main_lang}_corpus_counts.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mfnczMjmLxYu",
      "metadata": {
        "id": "mfnczMjmLxYu"
      },
      "outputs": [],
      "source": [
        "# Create Vocab Files\n",
        "langs = [\"en\",\"es\",\"ar\"]\n",
        "for lang in langs:\n",
        "  with open(f\"{path_to_lang_vocab_lists}/{lang}_50k.txt\", 'r') as in_file:\n",
        "    with open(f\"{path_to_lang_vocab_lists}/{lang}_5k.txt\", 'w') as out_file:\n",
        "      out_file.write(\"\\n\".join([line.split()[0] for line in in_file.read().split(\"\\n\")[:5000]]))\n",
        "\n",
        "freqs = {\"en\": {}, \"es\": {}}\n",
        "for lang in [\"en\",\"es\"]:\n",
        "  if main_lang == \"en\":\n",
        "    sub_langs = [\"en\", \"es_en_DEEP\", \"es_en_DEEPL\", \"es_en_GOOGLE\", \"es_en_TRANSFORMERS\", \\\n",
        "                  \"ar_en_DEEP\", \"ar_en_DEEPL\", \"ar_en_GOOGLE\", \"ar_en_TRANSFORMERS\"]\n",
        "  elif main_lang == \"es\":\n",
        "    sub_langs = [\"es\", \"en_es_DEEP\", \"en_es_DEEPL\", \"en_es_GOOGLE\", \"en_es_TRANSFORMERS\"]\n",
        "\n",
        "  for sub_lang in sub_langs:\n",
        "    with open(f\"{path_to_pickles}/{sub_lang}_spacy.pickle\", 'rb') as f:\n",
        "      docs = [spacy.tokens.Doc(nlp.vocab).from_json(doc) for doc in pickle.load(f)]\n",
        "\n",
        "    for doc in docs:\n",
        "      for token in doc:\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", token.text)\n",
        "        if text.strip() != \"\":\n",
        "          if not token.text in freqs[lang]:\n",
        "            freqs[lang][token.text] = 0\n",
        "          freqs[lang][token.text] += 1\n",
        "# Sort dicts by value\n",
        "for lang in freqs.keys():\n",
        "  words = list({k: v for k, v in sorted(freqs[lang].items(), key=lambda item: item[1], reverse=True)}.keys())[:5000]\n",
        "  with open(f\"{path_to_lang_vocab_lists}/political_{lang}_5k.txt\", 'w') as f:\n",
        "    f.write(\"\\n\".join(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_jw_32MbJaTD",
      "metadata": {
        "id": "_jw_32MbJaTD"
      },
      "outputs": [],
      "source": [
        "# Score rarity for sentences\n",
        "\n",
        "# General tokens from https://github.com/hermitdave/FrequencyWords\n",
        "with open(f\"{path_to_lang_vocab_lists}/{main_lang}_5k.txt\", 'r') as f:\n",
        "  general_common_tokens = set(f.read().split(\"\\n\"))\n",
        "with open(f\"{path_to_lang_vocab_lists}/political_{main_lang}_5k.txt\", 'r') as f:\n",
        "  genre_common_tokens = set(f.read().split(\"\\n\"))\n",
        "\n",
        "if main_lang == \"en\":\n",
        "  langs = [\"en\", \"es_en_DEEP\", \"es_en_DEEPL\", \"es_en_GOOGLE\", \"es_en_TRANSFORMERS\", \\\n",
        "               \"ar_en_DEEP\", \"ar_en_DEEPL\", \"ar_en_GOOGLE\", \"ar_en_TRANSFORMERS\"]\n",
        "elif main_lang == \"es\":\n",
        "  langs = [\"es\", \"en_es_DEEP\", \"en_es_DEEPL\", \"en_es_GOOGLE\", \"en_es_TRANSFORMERS\"]\n",
        "\n",
        "master_df = pd.read_csv(path_to_data_master)\n",
        "out_df = pd.DataFrame(data={\"id\":master_df[\"id\"]})\n",
        "\n",
        "for lang in langs:\n",
        "  with open(f\"{path_to_pickles}/{lang}_spacy.pickle\", 'rb') as f:\n",
        "    df[lang] = [spacy.tokens.Doc(nlp.vocab).from_json(doc) for doc in pickle.load(f)]\n",
        "\n",
        "for lang in langs:\n",
        "  general_rare_proportion, genre_rare_proportion = zip(*list(df[lang].apply(measure_rarity)))\n",
        "  out_df[f\"{lang}_general_rare_proportion\"] = general_rare_proportion\n",
        "  out_df[f\"{lang}_genre_rare_proportion\"] = genre_rare_proportion\n",
        "\n",
        "out_df.to_csv(f\"{path_to_analysis_data}/{main_lang}_rarity.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jc6cvfR2euIC",
      "metadata": {
        "id": "Jc6cvfR2euIC"
      },
      "outputs": [],
      "source": [
        "# Compare overall averages of differences between corpora in rarity score\n",
        "main_lang = \"es\"\n",
        "\n",
        "if main_lang == \"en\":\n",
        "  langs = [\"es_en_DEEP\", \"es_en_DEEPL\", \"es_en_GOOGLE\", \"es_en_TRANSFORMERS\", \\\n",
        "               \"ar_en_DEEP\", \"ar_en_DEEPL\", \"ar_en_GOOGLE\", \"ar_en_TRANSFORMERS\"]\n",
        "elif main_lang == \"es\":\n",
        "  langs = [\"en_es_DEEP\", \"en_es_DEEPL\", \"en_es_GOOGLE\", \"en_es_TRANSFORMERS\"]\n",
        "\n",
        "df = pd.read_csv(f\"{path_to_analysis_data}/{main_lang}_rarity.csv\")\n",
        "\n",
        "for measurement in [\"general\", \"genre\"]:\n",
        "  print(f\"{measurement}:\")\n",
        "  for lang in langs:\n",
        "    print(f\"{lang}: {round((df.apply(lambda row: row[f'{main_lang}_{measurement}_rare_proportion'] - row[f'{lang}_{measurement}_rare_proportion'], axis=1).mean()) * 100, 2)}%\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1xcB10NzyZKR",
      "metadata": {
        "id": "1xcB10NzyZKR"
      },
      "outputs": [],
      "source": [
        "# Arabic, SpaCy does not work for Arabic to the extent required, so we use Farasa instead, but it requires functions specific to Arabic\n",
        "\n",
        "def generate_arabic_info(text):\n",
        "  '''\n",
        "  Generates a dictionary for a sentence in Arabic using Farasa which contains 1) the text, 2) the part of speech tagged sentence, and 3) the lemmatized sentence.\n",
        "  Requires three global variables: i (0 at calling apply), num_rows (length of column), and start_time (time.time() at calling apply)\n",
        "  These global variables are required for progress reporting.\n",
        "\n",
        "  :param: text (str): the sentence/paragraph to be processed using Farasa.\n",
        "  :return: dictionary containing the text, the part of speech tagged sentence, and the lemmatized sentence\n",
        "  '''\n",
        "  global i\n",
        "  global num_rows\n",
        "  i += 1\n",
        "\n",
        "  if i % 500 == 0:\n",
        "      cur_time = time.time()\n",
        "      print(f\"{i / num_rows * 100}% done.\")\n",
        "      print(f\"Estimated time remaining: {((cur_time - start_time) / i) * (num_rows - i) / 60} minutes\")\n",
        "\n",
        "  info = {}\n",
        "  if type(text) == float:\n",
        "    return None\n",
        "  else:\n",
        "    payload = {\"text\": text, \"api_key\": api_key}\n",
        "    info[\"text\"] = text\n",
        "    info[\"pos\"] = pos_tagger.tag(text)\n",
        "    info[\"lemma\"] = stemmer.stem(text)\n",
        "    # print(text)\n",
        "    # url = \"https://farasa.qcri.org/webapi/pos/\"\n",
        "    # print(requests.post(url, data=payload).text)\n",
        "    # #info[\"pos\"] = json.loads(requests.post(url, data=payload).text)\n",
        "    # url = \"https://farasa.qcri.org/webapi/lemmatization/\"\n",
        "    # print(requests.post(url, data=payload).text)\n",
        "    # info[\"lemma\"] = json.loads(requests.post(url, data=payload).text)\n",
        "    return info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ptIykN9ktXx",
      "metadata": {
        "id": "6ptIykN9ktXx"
      },
      "outputs": [],
      "source": [
        "# Parallel Arabic Functions\n",
        "\n",
        "def parallelize_dataframe(df, func, n_cores, lang=\"ar\"):\n",
        "    '''\n",
        "    This function parallelizes a split of a Pandas dataframe for a given mapped function.\n",
        "\n",
        "    :param: df (Pandas DataFrame): the dataframe to apply the function to\n",
        "    :param: func (function): the function to be applied to the dataframe\n",
        "    :param: n_cores (int): the number of cores to be used in parallelization\n",
        "    :param: lang (str): the language of the text, default is \"ar\"\n",
        "\n",
        "    :return: Pandas DataFrame: the dataframe with the function applied to it\n",
        "    '''\n",
        "    df_split = np.array_split(df, n_cores)\n",
        "    pool = Pool(n_cores)\n",
        "    df = pd.concat(pool.map(func, df_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df\n",
        "\n",
        "def generate_arabic_info(text):\n",
        "  '''\n",
        "  Generates a dictionary for a sentence in Arabic using Farasa which contains 1) the text, 2) the part of speech tagged sentence, and 3) the lemmatized sentence.\n",
        "\n",
        "  :param: text (str): the sentence/paragraph to be processed using Farasa.\n",
        "  :return: dictionary containing the text, the part of speech tagged sentence, and the lemmatized sentence\n",
        "  '''\n",
        "  info = {}\n",
        "  if type(text) == float:\n",
        "    return None\n",
        "  else:\n",
        "    # payload = {\"text\": text, \"api_key\": api_key}\n",
        "    info[\"text\"] = text\n",
        "    info[\"pos\"] = pos_tagger.tag(text)\n",
        "    info[\"lemma\"] = stemmer.stem(text)\n",
        "    # print(text)\n",
        "    # url = \"https://farasa.qcri.org/webapi/pos/\"\n",
        "    # print(requests.post(url, data=payload).text)\n",
        "    # #info[\"pos\"] = json.loads(requests.post(url, data=payload).text)\n",
        "    # url = \"https://farasa.qcri.org/webapi/lemmatization/\"\n",
        "    # print(requests.post(url, data=payload).text)\n",
        "    # info[\"lemma\"] = json.loads(requests.post(url, data=payload).text)\n",
        "    return info\n",
        "\n",
        "def generate_arabic_info_from_df(df):\n",
        "    '''\n",
        "    Wrapper function for generate_arabic_info. Used to be passed to parallelize_dataframe.\n",
        "\n",
        "    :param: df (Pandas DataFrame): the dataframe to apply the function to\n",
        "    :return: Pandas DataFrame: the dataframe with the function applied to it\n",
        "    '''\n",
        "    global lang\n",
        "    content = df[lang].map(lambda x: generate_arabic_info(x))\n",
        "    df[f\"{lang}_info\"] = content\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xcH1sljrhr6E",
      "metadata": {
        "id": "xcH1sljrhr6E"
      },
      "outputs": [],
      "source": [
        "# Generate pickles for Arabic\n",
        "# Parallelized, must be ran on HPC\n",
        "\n",
        "langs = [\"ar\", \"en_ar_DEEP\", \"en_ar_DEEPL\", \"en_ar_GOOGLE\", \"en_ar_TRANSFORMERS\"]\n",
        "\n",
        "df = pd.read_csv(path_to_data_master)\n",
        "\n",
        "start = 0\n",
        "limit = 94\n",
        "total_time_start = time.time()\n",
        "results = []\n",
        "\n",
        "pos_tagger = FarasaPOSTagger()\n",
        "stemmer = FarasaStemmer()\n",
        "\n",
        "for lang in langs:\n",
        "    start = 0\n",
        "    limit = 94\n",
        "    total_time_start = time.time()\n",
        "    results = []\n",
        "    while start < len(df):\n",
        "        start_time = time.time()\n",
        "        results.append(parallelize_dataframe(df[start:start+limit], generate_arabic_info_from_df, limit, lang=lang))\n",
        "        end_time = time.time()\n",
        "        print(f'Batch of data of row range {start}-{start+limit} complete in {round(end_time-start_time, 2)} seconds')\n",
        "        print(f'{round(min((((start+limit) / len(df)) * 100), 100), 2)}% complete')\n",
        "        start+=limit\n",
        "\n",
        "    results_df = pd.concat(results)\n",
        "    total_time_end = time.time()\n",
        "    print(f'total time taken: {round(total_time_end - total_time_start,2)} second')\n",
        "\n",
        "    with open(f\"{path_to_pickles}/{lang}_spacy.pickle\", 'wb') as f:\n",
        "      pickle.dump(list(results_df[f\"{lang}_info\"]), f)\n",
        "      print(f\"Saved {lang}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-_6G34RHtE4O",
      "metadata": {
        "id": "-_6G34RHtE4O"
      },
      "outputs": [],
      "source": [
        "# Linguistic Analysis Functions overwritten for Farasa dicts\n",
        "\n",
        "def count_pos(doc, parts_of_speech=[r\"^NOUN$\", r\"^V$\"]):\n",
        "  '''\n",
        "  Count parts of speech in the sentence represented by doc.\n",
        "\n",
        "  :param: doc (dict): the document to be analyzed\n",
        "  :param: parts_of_speech ([Str]): an array of regex strings to define parts of speech to be counted, by default counts nouns and verbs\n",
        "\n",
        "  :return: an array of integers representing the counts of each part of speech in the sentence, respective to the order of the regex array\n",
        "  '''\n",
        "  if doc == None:\n",
        "      return [None] * len(parts_of_speech)\n",
        "  counts = [0] * len(parts_of_speech)\n",
        "  for token in doc[\"pos\"].split():\n",
        "    for i, part_of_speech in enumerate(parts_of_speech):\n",
        "      if len(token.split(\"/\")) > 1:\n",
        "        for cur_part_of_speech in token.split(\"/\")[1].split(\"+\"):\n",
        "          if re.match(part_of_speech, cur_part_of_speech):\n",
        "            counts[i] += 1\n",
        "  return counts\n",
        "\n",
        "def count_lemma(doc):\n",
        "  '''\n",
        "  Counts the number of unique lemmas in the sentence represented by doc.\n",
        "\n",
        "  :param: doc (dict): the document to be analyzed\n",
        "\n",
        "  :return: an integer representing the number of unique lemmas in the document\n",
        "  '''\n",
        "  return len(set([token for token in doc[\"lemma\"].split()])) if doc != None and doc[\"text\"] != \"\" else None\n",
        "\n",
        "def count_all_lemma(doc):\n",
        "  '''\n",
        "  Adds all lemmas and words to sets for each language in the dictionaries lemmas and words. Used to define total number of unique lemmas and words\n",
        "\n",
        "  :param: doc (dict): the document to be analyzed\n",
        "\n",
        "  :return: None (effect is addition to the sets in lemmas)\n",
        "  '''\n",
        "  if doc == None:\n",
        "    return\n",
        "  tokens = nltk.word_tokenize(doc[\"text\"])\n",
        "  stems = nltk.word_tokenize(doc[\"lemma\"])\n",
        "  for token in tokens:\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", token)\n",
        "    if text != \"\":\n",
        "      words[lang].add(token)\n",
        "  for stem in stems:\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", stem)\n",
        "    if text != \"\":\n",
        "      lemmas[lang].add(stem)\n",
        "\n",
        "def measure_rarity(doc):\n",
        "  '''\n",
        "  Measure the number of tokens in the text which are rare in both a general corpus and political (genre) corpus.\n",
        "\n",
        "  :param: doc (dict): the document to be analyzed\n",
        "\n",
        "  :return: int, int: the number of rare tokens in the sentence as compared to a general corpus and the number of rare tokens in the sentence as compared to a political corpus\n",
        "  '''\n",
        "  if doc == None:\n",
        "    return None, None\n",
        "  general_rare_token_count = 0\n",
        "  genre_rare_token_count = 0\n",
        "  punct_count = 0\n",
        "  tokens = nltk.word_tokenize(doc[\"text\"])\n",
        "  for token in tokens:\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", token)\n",
        "    if text != \"\":\n",
        "      if not token in general_common_tokens:\n",
        "        general_rare_token_count += 1\n",
        "      if not token in genre_common_tokens:\n",
        "        genre_rare_token_count += 1\n",
        "    else:\n",
        "      punct_count += 1\n",
        "  return general_rare_token_count / (len(tokens) - punct_count), genre_rare_token_count / (len(tokens) - punct_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "csS0e9OmuGti",
      "metadata": {
        "id": "csS0e9OmuGti"
      },
      "outputs": [],
      "source": [
        "# Count nouns, verbs, and lemmas in Arabic sentences\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "main_lang = \"ar\"\n",
        "\n",
        "if main_lang == \"ar\":\n",
        "  langs = [\"ar\", \"en_ar_DEEP\", \"en_ar_DEEPL\", \"en_ar_GOOGLE\", \"en_ar_TRANSFORMERS\"]\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for lang in langs:\n",
        "  with open(f\"{path_to_pickles}/{lang}_spacy.pickle\", 'rb') as f:\n",
        "    docs = pickle.load(f)\n",
        "    df[lang] = docs\n",
        "\n",
        "master_df = pd.read_csv(path_to_data_master)\n",
        "out_df = pd.DataFrame(data={\"id\":master_df[\"id\"]})\n",
        "\n",
        "for lang in langs:\n",
        "  noun_counts, verb_counts = zip(*list(df[lang].apply(count_pos)))\n",
        "  out_df[f\"{lang}_noun_counts\"] = noun_counts\n",
        "  out_df[f\"{lang}_verb_counts\"] = verb_counts\n",
        "  out_df[f\"{lang}_lemma_counts\"] = df[lang].apply(count_lemma)\n",
        "\n",
        "out_df.to_csv(f\"{path_to_analysis_data}/{main_lang}_counts.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MX59w9Det_MC",
      "metadata": {
        "id": "MX59w9Det_MC"
      },
      "outputs": [],
      "source": [
        "# Count unique lemmas and words in Arabic corpora\n",
        "\n",
        "lemmas = {lang: set() for lang in langs}\n",
        "words = {lang: set() for lang in langs}\n",
        "\n",
        "corpus_counts_df = pd.DataFrame(data={\"label\":[\"lemma\",\"word\"]})\n",
        "\n",
        "for lang in langs:\n",
        "  df[lang].apply(count_all_lemma)\n",
        "\n",
        "for lang in lemmas.keys():\n",
        "  corpus_counts_df[lang] = [len(lemmas[lang]), len(words[lang])]\n",
        "\n",
        "corpus_counts_df.to_csv(f\"{path_to_analysis_data}/{main_lang}_corpus_counts.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WyinHwjcuq7x",
      "metadata": {
        "id": "WyinHwjcuq7x"
      },
      "outputs": [],
      "source": [
        "# Generate common political vocab list for Arabic\n",
        "\n",
        "freqs = {\"ar\": {}}\n",
        "for lang in [\"ar\"]:\n",
        "\n",
        "  if main_lang == \"ar\":\n",
        "    sub_langs = [\"ar\", \"en_ar_DEEP\", \"en_ar_DEEPL\", \"en_ar_GOOGLE\", \"en_ar_TRANSFORMERS\"]\n",
        "\n",
        "  for sub_lang in sub_langs:\n",
        "    with open(f\"{path_to_pickles}/{sub_lang}_spacy.pickle\", 'rb') as f:\n",
        "      docs = pickle.load(f)\n",
        "\n",
        "    for doc in docs:\n",
        "      if doc == None:\n",
        "        continue\n",
        "      tokens = nltk.word_tokenize(doc[\"text\"])\n",
        "      for token in tokens:\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", token)\n",
        "        if text.strip() != \"\":\n",
        "          if not token in freqs[lang]:\n",
        "            freqs[lang][token] = 0\n",
        "          freqs[lang][token] += 1\n",
        "# Sort dicts by value\n",
        "for lang in freqs.keys():\n",
        "  words = list({k: v for k, v in sorted(freqs[lang].items(), key=lambda item: item[1], reverse=True)}.keys())[:5000]\n",
        "  with open(f\"{path_to_lang_vocab_lists}/political_{lang}_5k.txt\", 'w') as f:\n",
        "    f.write(\"\\n\".join(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fvnF-6skuUhW",
      "metadata": {
        "id": "fvnF-6skuUhW"
      },
      "outputs": [],
      "source": [
        "# Measure rarity in Arabic sentences\n",
        "\n",
        "if main_lang == \"ar\":\n",
        "  langs = [\"ar\", \"en_ar_DEEP\", \"en_ar_DEEPL\", \"en_ar_GOOGLE\", \"en_ar_TRANSFORMERS\"]\n",
        "\n",
        "# General tokens from https://github.com/hermitdave/FrequencyWords\n",
        "with open(f\"{path_to_lang_vocab_lists}/{main_lang}_5k.txt\", 'r') as f:\n",
        "  general_common_tokens = set(f.read().split(\"\\n\"))\n",
        "with open(f\"{path_to_lang_vocab_lists}/political_{main_lang}_5k.txt\", 'r') as f:\n",
        "  genre_common_tokens = set(f.read().split(\"\\n\"))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for lang in langs:\n",
        "  with open(f\"{path_to_pickles}/{lang}_spacy.pickle\", 'rb') as f:\n",
        "    df[lang] = pickle.load(f)\n",
        "\n",
        "master_df = pd.read_csv(\"Path to data_master.csv here\")\n",
        "out_df = pd.DataFrame(data={\"id\":master_df[\"id\"]})\n",
        "\n",
        "for lang in langs:\n",
        "  general_rare_proportion, genre_rare_proportion = zip(*list(df[lang].apply(measure_rarity)))\n",
        "  out_df[f\"{lang}_general_rare_proportion\"] = general_rare_proportion\n",
        "  out_df[f\"{lang}_genre_rare_proportion\"] = genre_rare_proportion\n",
        "\n",
        "out_df.to_csv(f\"{path_to_analysis_data}/{main_lang}_rarity.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3DfMwJL21cA2",
      "metadata": {
        "id": "3DfMwJL21cA2"
      },
      "outputs": [],
      "source": [
        "# Generate difference files for raw measurements within languages\n",
        "\n",
        "for lang in [\"en\", \"es\", \"ar\"]:\n",
        "  if lang == \"en\":\n",
        "    sub_langs = [\"es_en_DEEP\", \"es_en_DEEPL\", \"es_en_GOOGLE\", \"es_en_TRANSFORMERS\", \\\n",
        "                  \"ar_en_DEEP\", \"ar_en_DEEPL\", \"ar_en_GOOGLE\", \"ar_en_TRANSFORMERS\"]\n",
        "  if lang == \"es\":\n",
        "    sub_langs = [\"en_es_DEEP\", \"en_es_DEEPL\", \"en_es_GOOGLE\", \"en_es_TRANSFORMERS\"]\n",
        "  elif lang == \"ar\":\n",
        "    sub_langs = [\"en_ar_DEEP\", \"en_ar_DEEPL\", \"en_ar_GOOGLE\", \"en_ar_TRANSFORMERS\"]\n",
        "\n",
        "  df = pd.read_csv(f\"{path_to_analysis_data}/{lang}_counts.csv\")\n",
        "\n",
        "  master_df = pd.read_csv(path_to_data_master)\n",
        "  out_df = pd.DataFrame(data={\"id\":master_df[\"id\"]})\n",
        "\n",
        "  for sub_lang in sub_langs:\n",
        "    out_df[f\"{sub_lang}_difference_noun_counts\"] = df[f\"{sub_lang}_noun_counts\"] - df[f\"{lang}_noun_counts\"]\n",
        "    out_df[f\"{sub_lang}_difference_verb_counts\"] = df[f\"{sub_lang}_verb_counts\"] - df[f\"{lang}_verb_counts\"]\n",
        "    out_df[f\"{sub_lang}_difference_lemma_counts\"] = df[f\"{sub_lang}_lemma_counts\"] - df[f\"{lang}_lemma_counts\"]\n",
        "\n",
        "  out_df.to_csv(f\"{path_to_analysis_data}/{lang}_counts_difference.csv\", index=False)\n",
        "\n",
        "  df = pd.read_csv(f\"{path_to_analysis_data}/{lang}_rarity.csv\")\n",
        "\n",
        "  master_df = pd.read_csv(path_to_data_master)\n",
        "  out_df = pd.DataFrame(data={\"id\":master_df[\"id\"]})\n",
        "\n",
        "  for sub_lang in sub_langs:\n",
        "    out_df[f\"{sub_lang}_difference_general_rare_proportion\"] = df[f\"{sub_lang}_general_rare_proportion\"] - df[f\"{lang}_general_rare_proportion\"]\n",
        "    out_df[f\"{sub_lang}_difference_genre_rare_proportion\"] = df[f\"{sub_lang}_genre_rare_proportion\"] - df[f\"{lang}_genre_rare_proportion\"]\n",
        "\n",
        "  out_df.to_csv(f\"{path_to_analysis_data}/{lang}_rarity_difference.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dj7ZWAa352L",
      "metadata": {
        "id": "6dj7ZWAa352L"
      },
      "outputs": [],
      "source": [
        "# Generate difference files for raw measurements between languages\n",
        "\n",
        "for lang in [\"en\", \"es\", \"ar\"]:\n",
        "  if lang == \"en\":\n",
        "    sub_langs = [[\"en_es_DEEP\", \"en_es_DEEPL\", \"en_es_GOOGLE\", \"en_es_TRANSFORMERS\"], \\\n",
        "                 [\"en_ar_DEEP\", \"en_ar_DEEPL\", \"en_ar_GOOGLE\", \"en_ar_TRANSFORMERS\"]]\n",
        "  elif lang == \"es\":\n",
        "    sub_langs = [[\"es_en_DEEP\", \"es_en_DEEPL\", \"es_en_GOOGLE\", \"es_en_TRANSFORMERS\"]]\n",
        "  elif lang == \"ar\":\n",
        "    sub_langs = [[\"ar_en_DEEP\", \"ar_en_DEEPL\", \"ar_en_GOOGLE\", \"ar_en_TRANSFORMERS\"]]\n",
        "\n",
        "  origin_df = pd.read_csv(f\"{path_to_analysis_data}/{lang}_counts.csv\")\n",
        "\n",
        "  master_df = pd.read_csv(path_to_data_master)\n",
        "  out_df = pd.DataFrame(data={\"id\":master_df[\"id\"]})\n",
        "\n",
        "  for to_langs in sub_langs:\n",
        "    to_df = pd.read_csv(f\"{path_to_analysis_data}/{to_langs[0].split('_')[1]}_counts.csv\")\n",
        "\n",
        "    for to_lang in to_langs:\n",
        "      out_df[f\"{to_lang}_difference_noun_counts\"] = to_df[f\"{to_lang}_noun_counts\"] - origin_df[f\"{lang}_noun_counts\"]\n",
        "      out_df[f\"{to_lang}_difference_verb_counts\"] = to_df[f\"{to_lang}_verb_counts\"] - origin_df[f\"{lang}_verb_counts\"]\n",
        "      out_df[f\"{to_lang}_difference_lemma_counts\"] = to_df[f\"{to_lang}_lemma_counts\"] - origin_df[f\"{lang}_lemma_counts\"]\n",
        "\n",
        "  out_df.to_csv(f\"{path_to_analysis_data}/from_{lang}_counts_difference.csv\", index=False)\n",
        "\n",
        "  origin_df = pd.read_csv(f\"{path_to_analysis_data}/{lang}_rarity.csv\")\n",
        "\n",
        "  master_df = pd.read_csv(path_to_data_master)\n",
        "  out_df = pd.DataFrame(data={\"id\":master_df[\"id\"]})\n",
        "\n",
        "  for to_langs in sub_langs:\n",
        "    to_df = pd.read_csv(f\"{path_to_analysis_data}/{to_langs[0].split('_')[1]}_rarity.csv\")\n",
        "\n",
        "    for to_lang in to_langs:\n",
        "      out_df[f\"{to_lang}_difference_general_rare_proportion\"] = to_df[f\"{to_lang}_general_rare_proportion\"] - origin_df[f\"{lang}_general_rare_proportion\"]\n",
        "      out_df[f\"{to_lang}_difference_genre_rare_proportion\"] = to_df[f\"{to_lang}_genre_rare_proportion\"] - origin_df[f\"{lang}_genre_rare_proportion\"]\n",
        "\n",
        "  out_df.to_csv(f\"{path_to_analysis_data}/from_{lang}_rarity_difference.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
