{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlb0btSOvy_e",
    "outputId": "8d096d57-41dc-4771-9703-b7baa7a08320",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.9/site-packages (4.45.0.dev0)\n",
      "Requirement already satisfied: pandas in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.0.3)\n",
      "Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.9/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.9/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/bbov/salsarra/training/ConfliBERT/transformers/src/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results saved to en_ar_DEEPL_binary_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers pandas\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Function to load the model and tokenizer either from a local directory or Hugging Face\n",
    "def load_model(model_name_or_path):\n",
    "    if os.path.isdir(model_name_or_path):\n",
    "        # Load model and tokenizer from local directory\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "    else:\n",
    "        # Load model and tokenizer from Hugging Face\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to split long sentences into chunks of max 512 tokens\n",
    "def chunk_text(text, tokenizer, max_length):\n",
    "    # Tokenize the text and get token IDs\n",
    "    tokens = tokenizer(text, truncation=False, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    # Split the tokens into chunks of max_length\n",
    "    chunks = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]\n",
    "    # Decode each chunk back into text\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "# Function to classify text with chunking\n",
    "def classify_text_with_chunks(model_name_or_path, input_tsv, sentence_column, output_csv):\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer, model = load_model(model_name_or_path)\n",
    "    \n",
    "    # Load the text data from TSV\n",
    "    data = pd.read_csv(input_tsv, sep='\\t')\n",
    "    \n",
    "    # Prepare the pipeline\n",
    "    classification_pipeline = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        truncation=True,  # Ensure the tokens are truncated if too long\n",
    "        max_length=512,  # Truncate to the model's maximum token limit\n",
    "        return_all_scores=False,  # Only returns the label with the highest score\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    # Classify each sentence (with chunking if needed)\n",
    "    results = []\n",
    "    for sentence, identifier in zip(data[sentence_column], data['id']):\n",
    "        # Split sentence into chunks if it's too long\n",
    "        sentence_chunks = chunk_text(sentence, tokenizer, max_length=512)\n",
    "        chunk_results = []\n",
    "        for chunk in sentence_chunks:\n",
    "            # Pass the chunk as a string to the classification pipeline\n",
    "            result = classification_pipeline(chunk)[0]\n",
    "            binary_label = 0 if result['label'] == 'LABEL_0' else 1\n",
    "            chunk_results.append((binary_label, result['score']))\n",
    "        \n",
    "        # Aggregate chunk results by averaging probabilities or labels\n",
    "        avg_score = sum([score for _, score in chunk_results]) / len(chunk_results)\n",
    "        avg_label = round(sum([label for label, _ in chunk_results]) / len(chunk_results))  # Majority voting for label\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": identifier,\n",
    "            sentence_column: sentence,\n",
    "            \"Classification\": avg_label,\n",
    "            \"Prediction Score\": avg_score  # Changed from \"Probability Score\" to \"Prediction Score\"\n",
    "        })\n",
    "    \n",
    "    # Convert results to DataFrame and merge with original data\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_df = pd.merge(data, results_df[['id', 'Classification', 'Prediction Score']], on='id')\n",
    "    \n",
    "    # Save the results to CSV\n",
    "    output_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Classification results saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "# Replace 'your_local_model_directory' with the path to your local directory, or use Hugging Face model name\n",
    "model_name_or_path = \"/scratch/bbov/aalshammari/COLING/Training/ConfliBERT/outputs/en_ar_DEEPL_binary/best_model\"  # Can be a local directory or Hugging Face model link\n",
    "input_tsv = 'en_ar_DEEPL.tsv'  # Your input file path\n",
    "sentence_column = 'en_ar_DEEPL'  # Replace with the actual column name containing sentences\n",
    "output_csv = 'en_ar_DEEPL_binary_predictions.csv'  # Your output file path\n",
    "\n",
    "# Call the function\n",
    "classify_text_with_chunks(model_name_or_path, input_tsv, sentence_column, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
