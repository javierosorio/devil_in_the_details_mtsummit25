{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMOlSVBVybvSd+PN59xNYrY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlb0btSOvy_e","executionInfo":{"status":"ok","timestamp":1725804996702,"user_tz":-180,"elapsed":530497,"user":{"displayName":"Sultan Alsarra","userId":"08400289245164006734"}},"outputId":"8d096d57-41dc-4771-9703-b7baa7a08320"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Classification results saved to classification_results_with_chunks.csv\n"]}],"source":["# Install necessary libraries\n","!pip install transformers pandas\n","\n","# Import necessary libraries\n","import os\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import pipeline\n","import torch\n","\n","# Function to load the model and tokenizer either from a local directory or Hugging Face\n","def load_model(model_name_or_path):\n","    if os.path.isdir(model_name_or_path):\n","        # Load model and tokenizer from local directory\n","        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n","    else:\n","        # Load model and tokenizer from Hugging Face\n","        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","        model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n","    return tokenizer, model\n","\n","# Function to split long sentences into chunks of max 512 tokens\n","def chunk_text(text, tokenizer, max_length):\n","    # Tokenize the text and get token IDs\n","    tokens = tokenizer(text, truncation=False, return_tensors=\"pt\")[\"input_ids\"][0]\n","    # Split the tokens into chunks of max_length\n","    chunks = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]\n","    # Decode each chunk back into text\n","    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n","\n","# Function to classify text with chunking\n","def classify_text_with_chunks(model_name_or_path, input_tsv, sentence_column, output_csv):\n","    # Load the model and tokenizer\n","    tokenizer, model = load_model(model_name_or_path)\n","\n","    # Load the text data from TSV\n","    data = pd.read_csv(input_tsv, sep='\\t')\n","\n","    # Prepare the pipeline\n","    classification_pipeline = pipeline(\n","        \"text-classification\",\n","        model=model,\n","        tokenizer=tokenizer,\n","        truncation=True,  # Ensure the tokens are truncated if too long\n","        max_length=512,  # Truncate to the model's maximum token limit\n","        return_all_scores=False,  # Only returns the label with the highest score\n","        device=0 if torch.cuda.is_available() else -1\n","    )\n","\n","    # Classify each sentence (with chunking if needed)\n","    results = []\n","    for sentence, identifier in zip(data[sentence_column], data['id']):\n","        # Split sentence into chunks if it's too long\n","        sentence_chunks = chunk_text(sentence, tokenizer, max_length=512)\n","        chunk_results = []\n","        for chunk in sentence_chunks:\n","            # Pass the chunk as a string to the classification pipeline\n","            result = classification_pipeline(chunk)[0]\n","            binary_label = 0 if result['label'] == 'LABEL_0' else 1\n","            chunk_results.append((binary_label, result['score']))\n","\n","        # Aggregate chunk results by averaging probabilities or labels\n","        avg_score = sum([score for _, score in chunk_results]) / len(chunk_results)\n","        avg_label = round(sum([label for label, _ in chunk_results]) / len(chunk_results))  # Majority voting for label\n","\n","        results.append({\n","            \"id\": identifier,\n","            sentence_column: sentence,\n","            \"Classification\": avg_label,\n","            \"Prediction Score\": avg_score  # Changed from \"Probability Score\" to \"Prediction Score\"\n","        })\n","\n","    # Convert results to DataFrame and merge with original data\n","    results_df = pd.DataFrame(results)\n","    output_df = pd.merge(data, results_df[['id', 'Classification', 'Prediction Score']], on='id')\n","\n","    # Save the results to CSV\n","    output_df.to_csv(output_csv, index=False)\n","    print(f\"Classification results saved to {output_csv}\")\n","\n","# Example usage\n","# Replace 'your_local_model_directory' with the path to your local directory, or use Hugging Face model name\n","model_name_or_path = \"eventdata-utd/conflibert-binary-classification\"  # Can be a local directory or Hugging Face model link\n","input_tsv = 'test_sentences.tsv'  # Your input file path\n","sentence_column = 'en'  # Replace with the actual column name containing sentences\n","output_csv = 'classification_results_with_chunks.csv'  # Your output file path\n","\n","# Call the function\n","classify_text_with_chunks(model_name_or_path, input_tsv, sentence_column, output_csv)\n"]}]}